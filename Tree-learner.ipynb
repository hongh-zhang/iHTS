{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5049ef-66b7-4593-81c3-124a06a50333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 97 descriptor functions\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import typing\n",
    "from typing import List, Iterable\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import rdkit\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import os, glob, gc\n",
    "\n",
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "SEED = 0\n",
    "P_INIT = 0.10\n",
    "P_ITER = 0.05\n",
    "N_ITER = 5\n",
    "P_EXPLORE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0870852e-9a0e-4635-af80-daa0b088df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.99560546875\n"
     ]
    }
   ],
   "source": [
    "def cuda_mem():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print((t-a)/1024/1024/1024)\n",
    "cuda_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea2fa55-bf98-4178-8825-1aa56dd12f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/596/data.pkl\",\"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "fp_ls = data[\"fp_ls\"][:20000]\n",
    "y = np.array(data[\"activity_ls\"], dtype=float)[:20000]\n",
    "\n",
    "# initial picks using RDkit\n",
    "init_idxs = random_pick(fp_ls, int(P_INIT*len(fp_ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d521472-6252-42b4-872a-32cf0009dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_nodes = data['idx_to_nodes']\n",
    "node_to_hierarchy = data['node_to_hierarchy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3e989f-0814-47eb-b834-068f6d7045ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(y: np.array):\n",
    "    return sum(y) / len(y)\n",
    "def recovery_rate(y: np.array, idxs: Iterable):\n",
    "    return sum(y[idxs]) / sum(y)\n",
    "    \n",
    "def iHTS(learner, y, init_idxs, n_iter=N_ITER, p_iter=P_ITER):\n",
    "    #\n",
    "    results = []\n",
    "    \n",
    "    # initialize idxs\n",
    "    idxs = Indice(len(y))\n",
    "    \n",
    "    # observe initial set\n",
    "    idxs.add(init_idxs)\n",
    "    learner.observe(init_idxs, y[init_idxs])\n",
    "    results.append(idxs.sampled)\n",
    "    \n",
    "    print(f\"0 | {P_INIT:.2f}: {recovery_rate(y, idxs.sampled):.4f}\")\n",
    "    \n",
    "    # iteratively sample the library\n",
    "    for it in range(n_iter):\n",
    "        \n",
    "        sample_idxs = learner.select(idxs.unsampled, int(p_iter * len(y)))\n",
    "        learner.observe(sample_idxs, y[sample_idxs])\n",
    "        idxs.add(sample_idxs)\n",
    "\n",
    "        results.append(idxs.sampled)\n",
    "        \n",
    "        print(f\"{it+1} | {P_INIT+P_ITER*(it+1):.2f}: {recovery_rate(y, idxs.sampled):.4f}\")\n",
    "\n",
    "    results.append(idxs.unsampled)\n",
    "    print(\"\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777c7b9a-681b-424e-abb7-cb3577eae844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def common_hierarchy(i, j, idx_to_nodes, node_to_hierarchy):\n",
    "#     common_parents = list(set(idx_to_nodes[i]) & set(idx_to_nodes[i]))\n",
    "#     if not common_parents:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         common_lowest_parents = common_parents[0]\n",
    "#         return node_to_hierarchy[common_lowest_parents]\n",
    "\n",
    "class ScaffoldTreeKernel(gpytorch.kernels.Kernel):\n",
    "    is_stationary = True\n",
    "\n",
    "    def set_tree_param(self, weights=[0,0,0.1,0.2,0.3,0.4]):\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float16).cuda()\n",
    "    \n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        size = x1.shape[0]\n",
    "        return torch.stack([(((i==x2)&(i!=0)) * self.weights).sum(dim=1) for i in x1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56fdd254-f0f0-4469-89c6-2555c3a2a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffoldkernel = ScaffoldTreeKernel()\n",
    "scaffoldkernel.set_tree_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f09c3e-0282-40cf-a3a8-dcd03228ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy\n",
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "\n",
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = UnwhitenedVariationalStrategy(\n",
    "            self, train_x, variational_distribution, learn_inducing_locations=False, jitter_val=1e-6\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(scaffoldkernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020323c7-ead7-4550-9419-9f9106b12c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP_learner(Learner):\n",
    "    \"\"\"Thompson sampling learner based on gaussian process\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        SIZE=20000\n",
    "        idx_to_nodes = data['idx_to_nodes']\n",
    "        node_to_hierarchy = data['node_to_hierarchy']\n",
    "        \n",
    "        self.X_master = np.zeros((SIZE,6),dtype=int)\n",
    "        for idx, nodes in idx_to_nodes.items():\n",
    "            if idx<SIZE:\n",
    "                for node in nodes:\n",
    "                    self.X_master[idx,node_to_hierarchy[node]] = node\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "        self.state_dict = None\n",
    "        \n",
    "    def observe(self, idxs: List[int], activities: np.array, n_iter=50) -> None:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # assemble dataset\n",
    "        if self.X_train is None:\n",
    "            self.X_train = self.X_master[idxs]\n",
    "            self.y_train = activities.reshape(-1,1)\n",
    "        else:\n",
    "            self.X_train = np.concatenate([self.X_train, self.X_master[idxs]], axis=0)\n",
    "            self.y_train = np.concatenate([self.y_train, activities.reshape(-1,1)], axis=0)\n",
    "\n",
    "        X_train = torch.tensor(self.X_train, dtype=torch.int).cuda()\n",
    "        y_train = torch.tensor(self.y_train, dtype=torch.float16).flatten().cuda()\n",
    "        \n",
    "        # train model\n",
    "\n",
    "        # initialize model\n",
    "        self.gp_model = GPClassificationModel(X_train)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # if self.state_dict is not None:\n",
    "        #     self.gp_model.load_state_dict(self.state_dict)\n",
    "        self.gp_model.cuda()\n",
    "        self.gp_model.train()\n",
    "        \n",
    "        self.likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "        self.likelihood.cuda()\n",
    "        self.likelihood.train()\n",
    "        \n",
    "        # optimizer and objective function\n",
    "        optimizer = torch.optim.Adam(self.gp_model.parameters(), lr=5e-3)\n",
    "        mll = gpytorch.mlls.VariationalELBO(self.likelihood, self.gp_model, y_train.numel())\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.gp_model(X_train)\n",
    "            loss = -mll(output, y_train)\n",
    "            loss.backward()\n",
    "            if i%10 == 0:\n",
    "                print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()), end='\\r')\n",
    "            optimizer.step()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()))\n",
    "\n",
    "        del optimizer, mll, loss, X_train, y_train\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # self.state_dict = self.gp_model.state_dict()\n",
    "        \n",
    "    def sample(self, X) -> np.array:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        self.gp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            f_preds = self.gp_model(X.cuda())\n",
    "        f_samples = f_preds.sample()\n",
    "        return f_samples.detach().cpu().numpy()\n",
    "    \n",
    "    def predict(self, idxs: List[int]) -> np.array:\n",
    "        return self.sample(torch.tensor(self.X_master[idxs], dtype=torch.float16))\n",
    "    \n",
    "    def select(self, idxs: List[int], n: int) -> List[int]:\n",
    "        return self.rank(idxs)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50f28a3-0c43-4293-b107-354901a7201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50/50 - Loss: 0.651\n",
      "0 | 0.10: 0.1740\n",
      "Iter 50/50 - Loss: 0.746\n",
      "1 | 0.15: 0.2121\n",
      "Iter 50/50 - Loss: 0.813\n",
      "2 | 0.20: 0.2480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gp_learner \u001b[38;5;241m=\u001b[39m GP_learner(data)\n\u001b[1;32m----> 2\u001b[0m gp_results \u001b[38;5;241m=\u001b[39m \u001b[43miHTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgp_learner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_idxs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36miHTS\u001b[1;34m(learner, y, init_idxs, n_iter, p_iter)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# iteratively sample the library\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m---> 23\u001b[0m     sample_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     learner\u001b[38;5;241m.\u001b[39mobserve(sample_idxs, y[sample_idxs])\n\u001b[0;32m     25\u001b[0m     idxs\u001b[38;5;241m.\u001b[39madd(sample_idxs)\n",
      "Cell \u001b[1;32mIn[11], line 84\u001b[0m, in \u001b[0;36mGP_learner.select\u001b[1;34m(self, idxs, n)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs: List[\u001b[38;5;28mint\u001b[39m], n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m[:n]\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\iHTS\\utils\\learner.py:24\u001b[0m, in \u001b[0;36mLearner.rank\u001b[1;34m(self, idxs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrank\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m---> 24\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     ranked_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(probs)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(idxs)[ranked_idxs])\n",
      "Cell \u001b[1;32mIn[11], line 81\u001b[0m, in \u001b[0;36mGP_learner.predict\u001b[1;34m(self, idxs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39marray:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_master\u001b[49m\u001b[43m[\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m, in \u001b[0;36mGP_learner.sample\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 76\u001b[0m     f_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m f_samples \u001b[38;5;241m=\u001b[39m f_preds\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f_samples\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:108\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[1;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    107\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:341\u001b[0m, in \u001b[0;36m_VariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Get q(f)\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, MultivariateNormal):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43minducing_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43minducing_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariational_dist_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariational_inducing_covar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariational_dist_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, Delta):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    350\u001b[0m         x, inducing_points, inducing_values\u001b[38;5;241m=\u001b[39mvariational_dist_u\u001b[38;5;241m.\u001b[39mmean, variational_inducing_covar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    351\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\variational\\unwhitened_variational_strategy.py:188\u001b[0m, in \u001b[0;36mUnwhitenedVariationalStrategy.forward\u001b[1;34m(self, x, inducing_points, inducing_values, variational_inducing_covar, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     left_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mean_diff, root_variational_covar], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m inv_products \u001b[38;5;241m=\u001b[39m \u001b[43minduc_induc_covar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43minduc_data_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(test_mean, inv_products[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Compute covariance\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2336\u001b[0m, in \u001b[0;36mLinearOperator.solve\u001b[1;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[0;32m   2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), \u001b[38;5;28;01mFalse\u001b[39;00m, right_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[0;32m   2335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2341\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\functions\\_solve.py:49\u001b[0m, in \u001b[0;36mSolve.forward\u001b[1;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mhas_left:\n\u001b[0;32m     48\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([left_tensor\u001b[38;5;241m.\u001b[39mmT, right_tensor], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     solves \u001b[38;5;241m=\u001b[39m \u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     res \u001b[38;5;241m=\u001b[39m solves[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, left_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) :]\n\u001b[0;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_tensor \u001b[38;5;241m@\u001b[39m res\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\functions\\_solve.py:21\u001b[0m, in \u001b[0;36m_solve\u001b[1;34m(linear_op, rhs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     20\u001b[0m     preconditioner \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39m_solve_preconditioner()\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:789\u001b[0m, in \u001b[0;36mLinearOperator._solve\u001b[1;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_solve\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    776\u001b[0m     rhs: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N C\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m     ],\n\u001b[0;32m    785\u001b[0m ]:\n\u001b[0;32m    786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;124;03m    TODO\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tridiag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cg_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tridiag_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_lanczos_quadrature_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\utils\\linear_cg.py:304\u001b[0m, in \u001b[0;36mlinear_cg\u001b[1;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[0;32m    299\u001b[0m residual_norm\u001b[38;5;241m.\u001b[39mmasked_fill_(rhs_is_zero, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    300\u001b[0m torch\u001b[38;5;241m.\u001b[39mlt(residual_norm, stop_updating_after, out\u001b[38;5;241m=\u001b[39mhas_converged)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10\u001b[39m, max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresidual_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (n_tridiag \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_tridiag_iter, max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    306\u001b[0m ):\n\u001b[0;32m    307\u001b[0m     tolerance_reached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gp_learner = GP_learner(data)\n",
    "gp_results = iHTS(gp_learner, y, init_idxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
