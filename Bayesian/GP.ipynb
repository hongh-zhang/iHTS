{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d96ff94-3a90-4c6c-9f83-83e7aed6473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 97 descriptor functions\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import typing\n",
    "from typing import List, Iterable\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import rdkit\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import os, glob\n",
    "\n",
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24b7e21-d5fb-4fab-8fb7-db3159c80225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\r"
     ]
    }
   ],
   "source": [
    "print(1, end='\\r')\n",
    "print(2, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4094216b-0c58-49b8-ae4b-6870b2581b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67129250-2892-4f18-b471-d0364338d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.729490280151367\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print((t-a)/1024/1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2e4e79-4c55-46c0-9802-9e9ea66fdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/596/data.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X = np.array(data['fp_ls'])\n",
    "y = np.array(data['activity_ls'])\n",
    "\n",
    "X = X[:2000]\n",
    "y = y[:2000]\n",
    "X = X.astype(float)\n",
    "y = y.reshape(-1,1).astype(float)\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.float16).to(device)\n",
    "y = torch.from_numpy(y).type(torch.float16).flatten().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5b9062-f654-4ab7-870c-689ba3ef8fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliLikelihood(\n",
       "  (quadrature): GaussHermiteQuadrature1D()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy\n",
    "\n",
    "\n",
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = UnwhitenedVariationalStrategy(\n",
    "            self, train_x, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(TanimotoKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(X)\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "\n",
    "#\n",
    "#likelihood = gpytorch.likelihoods.BetaLikelihood()\n",
    "\n",
    "model.cuda()\n",
    "likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1742e12-4f8b-4e58-a1cc-142a4d54e0ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n",
      "<bound method TorchDistributionMixin.shape of MultivariateNormal(loc: torch.Size([2000]))>\n",
      "torch.Size([2000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\mlls\\variational_elbo.py:77\u001b[0m, in \u001b[0;36mVariationalELBO.forward\u001b[1;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, variational_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    Calling this function will call the likelihood's :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariational_dist_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\mlls\\_approximate_mll.py:59\u001b[0m, in \u001b[0;36m_ApproximateMarginalLogLikelihood.forward\u001b[1;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m num_batch \u001b[38;5;241m=\u001b[39m approximate_dist_f\u001b[38;5;241m.\u001b[39mevent_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     58\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_likelihood_term(approximate_dist_f, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdiv(num_batch)\n\u001b[1;32m---> 59\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Add any additional registered loss terms\u001b[39;00m\n\u001b[0;32m     62\u001b[0m added_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(log_likelihood)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:161\u001b[0m, in \u001b[0;36m_VariationalStrategy.kl_divergence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mCompute the KL divergence between the variational inducing distribution :math:`q(\\mathbf u)`\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03mand the prior inducing distribution :math:`p(\\mathbf u)`.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mmax_preconditioner_size(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 161\u001b[0m     kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_divergence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariational_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior_distribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kl_divergence\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\distributions\\kl.py:191\u001b[0m, in \u001b[0;36mkl_divergence\u001b[1;34m(p, q)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo KL(p || q) is implemented for p type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and q type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m     )\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:412\u001b[0m, in \u001b[0;36mkl_mvn_mvn\u001b[1;34m(p_dist, q_dist)\u001b[0m\n\u001b[0;32m    410\u001b[0m inv_quad_rhs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mean_diffs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), root_p_covar], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    411\u001b[0m logdet_p_covar \u001b[38;5;241m=\u001b[39m p_covar\u001b[38;5;241m.\u001b[39mlogdet()\n\u001b[1;32m--> 412\u001b[0m trace_plus_inv_quad_form, logdet_q_covar \u001b[38;5;241m=\u001b[39m \u001b[43mq_covar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Compute the KL Divergence.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([logdet_q_covar, logdet_p_covar\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), trace_plus_inv_quad_form, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(mean_diffs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))])\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1764\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1761\u001b[0m probe_vectors, probe_vector_norms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probe_vectors_and_norms()\n\u001b[0;32m   1763\u001b[0m func \u001b[38;5;241m=\u001b[39m InvQuadLogdet\u001b[38;5;241m.\u001b[39mapply\n\u001b[1;32m-> 1764\u001b[0m inv_quad_term, pinvk_logdet \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecond_lt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprecond_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe_vector_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprecond_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1774\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m pinvk_logdet\n\u001b[0;32m   1775\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m logdet_term \u001b[38;5;241m+\u001b[39m logdet_p\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\functions\\_inv_quad_logdet.py:132\u001b[0m, in \u001b[0;36mInvQuadLogdet.forward\u001b[1;34m(ctx, representation_tree, precond_representation_tree, preconditioner, num_precond_args, inv_quad, probe_vectors, probe_vector_norms, *args)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m rhs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(rhs_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m solves, t_mat \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_random_probes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Final values to return\u001b[39;00m\n\u001b[0;32m    135\u001b[0m logdet_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(linear_op\u001b[38;5;241m.\u001b[39mbatch_shape, dtype\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:789\u001b[0m, in \u001b[0;36mLinearOperator._solve\u001b[1;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_solve\u001b[39m(\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    776\u001b[0m     rhs: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... N C\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m     ],\n\u001b[0;32m    785\u001b[0m ]:\n\u001b[0;32m    786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;124;03m    TODO\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_tridiag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cg_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tridiag_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_lanczos_quadrature_iterations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\linear_operator\\utils\\linear_cg.py:207\u001b[0m, in \u001b[0;36mlinear_cg\u001b[1;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[0;32m    204\u001b[0m residual_norm \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    205\u001b[0m has_converged \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlt(residual_norm, stop_updating_after)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_converged\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n_tridiag:\n\u001b[0;32m    208\u001b[0m     n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Skip the iteration!\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Otherwise, let's define precond_residual and curr_conjugate_vec\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# precon_residual{0} = M^-1 residual_{0}\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_iterations = 100\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the number of training datapoints\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, y.numel())\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(X)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(y.shape)\n",
    " \n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y)\n",
    "    loss.backward()\n",
    "    if i%10 == 9:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceade672-1964-4420-903e-ccd6e34a86d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5353,  0.0290,  0.7745,  ...,  1.3258, -1.1526, -0.7138],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32e45cf1-dfa0-4460-913b-221e08b94264",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_preds = model(X)\n",
    "y_preds = likelihood(model(X))\n",
    "\n",
    "f_mean = f_preds.mean\n",
    "f_samples = f_preds.sample(sample_shape=torch.Size((1000,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e700612-1ef0-490f-8d73-ad4ebdfa489f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3138,  0.2986, -0.0682,  ...,  0.2259, -2.5279, -0.6808],\n",
       "        [ 0.6883, -0.2564,  0.6097,  ...,  0.3862, -0.7299, -0.1721],\n",
       "        [ 0.1128, -0.3425,  0.0262,  ...,  0.8339, -1.5077, -0.6605],\n",
       "        ...,\n",
       "        [ 1.7282,  0.1455,  0.1376,  ..., -1.1487, -0.7588, -0.9380],\n",
       "        [ 0.2562, -0.6317,  1.1764,  ..., -0.7433, -0.8802,  0.0275],\n",
       "        [ 0.5238,  0.3733,  1.0226,  ...,  0.3846, -0.9686,  0.4370]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b164ab4-f1ac-4425-bf07-bfe76f5107c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 1., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c0f58b-7095-4fa5-a4b4-119c7d4021d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[model(X).sample().detach().cpu().numpy().argsort()].cpu().numpy()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e2142-da02-4344-8455-21d808285325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
